{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.0,
  "eval_steps": 500,
  "global_step": 13263,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.003769886149438287,
      "grad_norm": 0.3061670660972595,
      "learning_rate": 2.4685138539042822e-05,
      "loss": 5.7962,
      "step": 50
    },
    {
      "epoch": 0.007539772298876574,
      "grad_norm": 1.3591352701187134,
      "learning_rate": 4.9874055415617133e-05,
      "loss": 4.1176,
      "step": 100
    },
    {
      "epoch": 0.011309658448314861,
      "grad_norm": 1.5459612607955933,
      "learning_rate": 7.455919395465995e-05,
      "loss": 3.4873,
      "step": 150
    },
    {
      "epoch": 0.015079544597753148,
      "grad_norm": 1.4277251958847046,
      "learning_rate": 9.974811083123427e-05,
      "loss": 3.0661,
      "step": 200
    },
    {
      "epoch": 0.018849430747191434,
      "grad_norm": 1.5248225927352905,
      "learning_rate": 0.00012493702770780858,
      "loss": 2.9986,
      "step": 250
    },
    {
      "epoch": 0.022619316896629722,
      "grad_norm": 1.4982845783233643,
      "learning_rate": 0.00015012594458438288,
      "loss": 2.9538,
      "step": 300
    },
    {
      "epoch": 0.026389203046068007,
      "grad_norm": 1.4077234268188477,
      "learning_rate": 0.00017531486146095718,
      "loss": 2.9113,
      "step": 350
    },
    {
      "epoch": 0.030159089195506295,
      "grad_norm": 1.0447649955749512,
      "learning_rate": 0.00019998445515311675,
      "loss": 2.7258,
      "step": 400
    },
    {
      "epoch": 0.03392897534494458,
      "grad_norm": 1.0232439041137695,
      "learning_rate": 0.00019920721280895385,
      "loss": 2.8024,
      "step": 450
    },
    {
      "epoch": 0.03769886149438287,
      "grad_norm": 1.0859607458114624,
      "learning_rate": 0.00019842997046479093,
      "loss": 2.899,
      "step": 500
    },
    {
      "epoch": 0.041468747643821156,
      "grad_norm": 1.095328450202942,
      "learning_rate": 0.00019765272812062804,
      "loss": 2.8458,
      "step": 550
    },
    {
      "epoch": 0.045238633793259445,
      "grad_norm": 1.2334091663360596,
      "learning_rate": 0.00019687548577646512,
      "loss": 2.7753,
      "step": 600
    },
    {
      "epoch": 0.04900851994269773,
      "grad_norm": 1.1257872581481934,
      "learning_rate": 0.00019609824343230222,
      "loss": 2.7841,
      "step": 650
    },
    {
      "epoch": 0.052778406092136014,
      "grad_norm": 0.8747288584709167,
      "learning_rate": 0.0001953210010881393,
      "loss": 2.7824,
      "step": 700
    },
    {
      "epoch": 0.0565482922415743,
      "grad_norm": 0.8226062655448914,
      "learning_rate": 0.00019454375874397638,
      "loss": 2.7478,
      "step": 750
    },
    {
      "epoch": 0.06031817839101259,
      "grad_norm": 0.9918922781944275,
      "learning_rate": 0.00019376651639981348,
      "loss": 2.7521,
      "step": 800
    },
    {
      "epoch": 0.06408806454045088,
      "grad_norm": 1.0272547006607056,
      "learning_rate": 0.00019298927405565056,
      "loss": 2.7994,
      "step": 850
    },
    {
      "epoch": 0.06785795068988916,
      "grad_norm": 1.3267972469329834,
      "learning_rate": 0.00019221203171148764,
      "loss": 2.7593,
      "step": 900
    },
    {
      "epoch": 0.07162783683932746,
      "grad_norm": 1.2402325868606567,
      "learning_rate": 0.00019143478936732474,
      "loss": 2.7513,
      "step": 950
    },
    {
      "epoch": 0.07539772298876574,
      "grad_norm": 1.0411823987960815,
      "learning_rate": 0.00019065754702316182,
      "loss": 2.6425,
      "step": 1000
    },
    {
      "epoch": 0.07916760913820403,
      "grad_norm": 1.0347776412963867,
      "learning_rate": 0.0001898803046789989,
      "loss": 2.7111,
      "step": 1050
    },
    {
      "epoch": 0.08293749528764231,
      "grad_norm": 1.1176886558532715,
      "learning_rate": 0.000189103062334836,
      "loss": 2.6579,
      "step": 1100
    },
    {
      "epoch": 0.0867073814370806,
      "grad_norm": 1.0879520177841187,
      "learning_rate": 0.00018832581999067308,
      "loss": 2.7164,
      "step": 1150
    },
    {
      "epoch": 0.09047726758651889,
      "grad_norm": 1.3372429609298706,
      "learning_rate": 0.0001875485776465102,
      "loss": 2.7507,
      "step": 1200
    },
    {
      "epoch": 0.09424715373595717,
      "grad_norm": 0.8472229838371277,
      "learning_rate": 0.00018677133530234727,
      "loss": 2.6932,
      "step": 1250
    },
    {
      "epoch": 0.09801703988539547,
      "grad_norm": 1.1837708950042725,
      "learning_rate": 0.00018599409295818437,
      "loss": 2.6673,
      "step": 1300
    },
    {
      "epoch": 0.10178692603483375,
      "grad_norm": 1.2545506954193115,
      "learning_rate": 0.00018521685061402145,
      "loss": 2.6564,
      "step": 1350
    },
    {
      "epoch": 0.10555681218427203,
      "grad_norm": 1.4212861061096191,
      "learning_rate": 0.00018443960826985855,
      "loss": 2.6337,
      "step": 1400
    },
    {
      "epoch": 0.10932669833371032,
      "grad_norm": 0.6519841551780701,
      "learning_rate": 0.00018366236592569563,
      "loss": 2.5784,
      "step": 1450
    },
    {
      "epoch": 0.1130965844831486,
      "grad_norm": 1.0849852561950684,
      "learning_rate": 0.00018288512358153274,
      "loss": 2.714,
      "step": 1500
    },
    {
      "epoch": 0.1168664706325869,
      "grad_norm": 1.0148510932922363,
      "learning_rate": 0.00018210788123736982,
      "loss": 2.702,
      "step": 1550
    },
    {
      "epoch": 0.12063635678202518,
      "grad_norm": 1.0767732858657837,
      "learning_rate": 0.00018133063889320692,
      "loss": 2.7383,
      "step": 1600
    },
    {
      "epoch": 0.12440624293146348,
      "grad_norm": 1.114149808883667,
      "learning_rate": 0.000180553396549044,
      "loss": 2.6843,
      "step": 1650
    },
    {
      "epoch": 0.12817612908090176,
      "grad_norm": 1.1369870901107788,
      "learning_rate": 0.0001797761542048811,
      "loss": 2.6272,
      "step": 1700
    },
    {
      "epoch": 0.13194601523034005,
      "grad_norm": 0.841846227645874,
      "learning_rate": 0.00017899891186071818,
      "loss": 2.7147,
      "step": 1750
    },
    {
      "epoch": 0.13571590137977832,
      "grad_norm": 1.43205988407135,
      "learning_rate": 0.0001782216695165553,
      "loss": 2.6329,
      "step": 1800
    },
    {
      "epoch": 0.13948578752921661,
      "grad_norm": 1.0872454643249512,
      "learning_rate": 0.00017744442717239237,
      "loss": 2.7116,
      "step": 1850
    },
    {
      "epoch": 0.1432556736786549,
      "grad_norm": 1.2279177904129028,
      "learning_rate": 0.00017666718482822947,
      "loss": 2.6734,
      "step": 1900
    },
    {
      "epoch": 0.1470255598280932,
      "grad_norm": 1.3583998680114746,
      "learning_rate": 0.00017588994248406655,
      "loss": 2.5301,
      "step": 1950
    },
    {
      "epoch": 0.15079544597753147,
      "grad_norm": 1.3589260578155518,
      "learning_rate": 0.00017511270013990363,
      "loss": 2.6734,
      "step": 2000
    },
    {
      "epoch": 0.15456533212696977,
      "grad_norm": 1.0816460847854614,
      "learning_rate": 0.00017433545779574073,
      "loss": 2.623,
      "step": 2050
    },
    {
      "epoch": 0.15833521827640806,
      "grad_norm": 0.9760193228721619,
      "learning_rate": 0.0001735582154515778,
      "loss": 2.6061,
      "step": 2100
    },
    {
      "epoch": 0.16210510442584633,
      "grad_norm": 1.1574398279190063,
      "learning_rate": 0.0001727809731074149,
      "loss": 2.646,
      "step": 2150
    },
    {
      "epoch": 0.16587499057528463,
      "grad_norm": 1.3359540700912476,
      "learning_rate": 0.000172003730763252,
      "loss": 2.5792,
      "step": 2200
    },
    {
      "epoch": 0.16964487672472292,
      "grad_norm": 1.0827064514160156,
      "learning_rate": 0.00017122648841908907,
      "loss": 2.6275,
      "step": 2250
    },
    {
      "epoch": 0.1734147628741612,
      "grad_norm": 1.2100497484207153,
      "learning_rate": 0.00017044924607492615,
      "loss": 2.6102,
      "step": 2300
    },
    {
      "epoch": 0.17718464902359948,
      "grad_norm": 1.18639075756073,
      "learning_rate": 0.00016967200373076325,
      "loss": 2.5684,
      "step": 2350
    },
    {
      "epoch": 0.18095453517303778,
      "grad_norm": 1.1773673295974731,
      "learning_rate": 0.00016889476138660033,
      "loss": 2.5864,
      "step": 2400
    },
    {
      "epoch": 0.18472442132247607,
      "grad_norm": 1.205592393875122,
      "learning_rate": 0.00016811751904243744,
      "loss": 2.5972,
      "step": 2450
    },
    {
      "epoch": 0.18849430747191434,
      "grad_norm": 1.120015263557434,
      "learning_rate": 0.00016734027669827452,
      "loss": 2.7021,
      "step": 2500
    },
    {
      "epoch": 0.19226419362135264,
      "grad_norm": 0.5031312704086304,
      "learning_rate": 0.00016656303435411162,
      "loss": 2.6622,
      "step": 2550
    },
    {
      "epoch": 0.19603407977079093,
      "grad_norm": 1.0830860137939453,
      "learning_rate": 0.0001657857920099487,
      "loss": 2.6758,
      "step": 2600
    },
    {
      "epoch": 0.1998039659202292,
      "grad_norm": 1.1171001195907593,
      "learning_rate": 0.0001650085496657858,
      "loss": 2.6428,
      "step": 2650
    },
    {
      "epoch": 0.2035738520696675,
      "grad_norm": 1.2423189878463745,
      "learning_rate": 0.00016423130732162288,
      "loss": 2.6205,
      "step": 2700
    },
    {
      "epoch": 0.2073437382191058,
      "grad_norm": 0.9757939577102661,
      "learning_rate": 0.00016345406497746,
      "loss": 2.6612,
      "step": 2750
    },
    {
      "epoch": 0.21111362436854406,
      "grad_norm": 1.3628956079483032,
      "learning_rate": 0.00016267682263329707,
      "loss": 2.6526,
      "step": 2800
    },
    {
      "epoch": 0.21488351051798235,
      "grad_norm": 1.355824589729309,
      "learning_rate": 0.00016189958028913417,
      "loss": 2.6488,
      "step": 2850
    },
    {
      "epoch": 0.21865339666742065,
      "grad_norm": 0.9088183045387268,
      "learning_rate": 0.00016112233794497125,
      "loss": 2.571,
      "step": 2900
    },
    {
      "epoch": 0.22242328281685894,
      "grad_norm": 1.2795993089675903,
      "learning_rate": 0.00016034509560080835,
      "loss": 2.5797,
      "step": 2950
    },
    {
      "epoch": 0.2261931689662972,
      "grad_norm": 1.1993305683135986,
      "learning_rate": 0.00015956785325664543,
      "loss": 2.5868,
      "step": 3000
    },
    {
      "epoch": 0.2299630551157355,
      "grad_norm": 0.7221764922142029,
      "learning_rate": 0.00015879061091248254,
      "loss": 2.6204,
      "step": 3050
    },
    {
      "epoch": 0.2337329412651738,
      "grad_norm": 1.3539379835128784,
      "learning_rate": 0.00015801336856831962,
      "loss": 2.5526,
      "step": 3100
    },
    {
      "epoch": 0.23750282741461207,
      "grad_norm": 1.2083485126495361,
      "learning_rate": 0.00015723612622415672,
      "loss": 2.6768,
      "step": 3150
    },
    {
      "epoch": 0.24127271356405036,
      "grad_norm": 1.252117395401001,
      "learning_rate": 0.0001564588838799938,
      "loss": 2.5982,
      "step": 3200
    },
    {
      "epoch": 0.24504259971348866,
      "grad_norm": 1.2638379335403442,
      "learning_rate": 0.00015568164153583088,
      "loss": 2.628,
      "step": 3250
    },
    {
      "epoch": 0.24881248586292695,
      "grad_norm": 0.9185910820960999,
      "learning_rate": 0.00015490439919166798,
      "loss": 2.5788,
      "step": 3300
    },
    {
      "epoch": 0.2525823720123652,
      "grad_norm": 0.9070888161659241,
      "learning_rate": 0.00015412715684750506,
      "loss": 2.6851,
      "step": 3350
    },
    {
      "epoch": 0.2563522581618035,
      "grad_norm": 1.0714627504348755,
      "learning_rate": 0.00015334991450334214,
      "loss": 2.5573,
      "step": 3400
    },
    {
      "epoch": 0.2601221443112418,
      "grad_norm": 0.7560367584228516,
      "learning_rate": 0.00015257267215917924,
      "loss": 2.5861,
      "step": 3450
    },
    {
      "epoch": 0.2638920304606801,
      "grad_norm": 0.8043314218521118,
      "learning_rate": 0.00015179542981501632,
      "loss": 2.5182,
      "step": 3500
    },
    {
      "epoch": 0.2676619166101184,
      "grad_norm": 1.1761022806167603,
      "learning_rate": 0.0001510181874708534,
      "loss": 2.6561,
      "step": 3550
    },
    {
      "epoch": 0.27143180275955664,
      "grad_norm": 1.0030760765075684,
      "learning_rate": 0.0001502409451266905,
      "loss": 2.5574,
      "step": 3600
    },
    {
      "epoch": 0.27520168890899493,
      "grad_norm": 1.4879064559936523,
      "learning_rate": 0.00014946370278252758,
      "loss": 2.6304,
      "step": 3650
    },
    {
      "epoch": 0.27897157505843323,
      "grad_norm": 1.1088658571243286,
      "learning_rate": 0.0001486864604383647,
      "loss": 2.5581,
      "step": 3700
    },
    {
      "epoch": 0.2827414612078715,
      "grad_norm": 1.1419799327850342,
      "learning_rate": 0.00014790921809420177,
      "loss": 2.4644,
      "step": 3750
    },
    {
      "epoch": 0.2865113473573098,
      "grad_norm": 1.061676263809204,
      "learning_rate": 0.00014713197575003887,
      "loss": 2.6801,
      "step": 3800
    },
    {
      "epoch": 0.2902812335067481,
      "grad_norm": 1.3457127809524536,
      "learning_rate": 0.00014635473340587595,
      "loss": 2.5019,
      "step": 3850
    },
    {
      "epoch": 0.2940511196561864,
      "grad_norm": 1.167146921157837,
      "learning_rate": 0.00014557749106171305,
      "loss": 2.5688,
      "step": 3900
    },
    {
      "epoch": 0.29782100580562465,
      "grad_norm": 1.2025607824325562,
      "learning_rate": 0.00014480024871755013,
      "loss": 2.6921,
      "step": 3950
    },
    {
      "epoch": 0.30159089195506295,
      "grad_norm": 1.3793482780456543,
      "learning_rate": 0.00014402300637338724,
      "loss": 2.6228,
      "step": 4000
    },
    {
      "epoch": 0.30536077810450124,
      "grad_norm": 1.2578952312469482,
      "learning_rate": 0.00014324576402922432,
      "loss": 2.6029,
      "step": 4050
    },
    {
      "epoch": 0.30913066425393954,
      "grad_norm": 0.9953578114509583,
      "learning_rate": 0.00014246852168506142,
      "loss": 2.5004,
      "step": 4100
    },
    {
      "epoch": 0.31290055040337783,
      "grad_norm": 1.1715829372406006,
      "learning_rate": 0.0001416912793408985,
      "loss": 2.5774,
      "step": 4150
    },
    {
      "epoch": 0.3166704365528161,
      "grad_norm": 1.2041107416152954,
      "learning_rate": 0.0001409140369967356,
      "loss": 2.5526,
      "step": 4200
    },
    {
      "epoch": 0.32044032270225437,
      "grad_norm": 0.9574492573738098,
      "learning_rate": 0.00014013679465257268,
      "loss": 2.6267,
      "step": 4250
    },
    {
      "epoch": 0.32421020885169266,
      "grad_norm": 1.04930579662323,
      "learning_rate": 0.0001393595523084098,
      "loss": 2.4952,
      "step": 4300
    },
    {
      "epoch": 0.32798009500113096,
      "grad_norm": 0.9629228711128235,
      "learning_rate": 0.00013859785481113013,
      "loss": 2.5011,
      "step": 4350
    },
    {
      "epoch": 0.33174998115056925,
      "grad_norm": 1.1783757209777832,
      "learning_rate": 0.0001378206124669672,
      "loss": 2.5486,
      "step": 4400
    },
    {
      "epoch": 0.33551986730000755,
      "grad_norm": 0.9401504993438721,
      "learning_rate": 0.0001370433701228043,
      "loss": 2.542,
      "step": 4450
    },
    {
      "epoch": 0.33928975344944584,
      "grad_norm": 1.259564757347107,
      "learning_rate": 0.00013626612777864139,
      "loss": 2.5463,
      "step": 4500
    },
    {
      "epoch": 0.34305963959888414,
      "grad_norm": 1.15648353099823,
      "learning_rate": 0.0001354888854344785,
      "loss": 2.5464,
      "step": 4550
    },
    {
      "epoch": 0.3468295257483224,
      "grad_norm": 1.4749289751052856,
      "learning_rate": 0.00013471164309031557,
      "loss": 2.6364,
      "step": 4600
    },
    {
      "epoch": 0.35059941189776067,
      "grad_norm": 1.210418939590454,
      "learning_rate": 0.00013393440074615267,
      "loss": 2.5731,
      "step": 4650
    },
    {
      "epoch": 0.35436929804719897,
      "grad_norm": 1.044622778892517,
      "learning_rate": 0.00013315715840198975,
      "loss": 2.5191,
      "step": 4700
    },
    {
      "epoch": 0.35813918419663726,
      "grad_norm": 0.9714714884757996,
      "learning_rate": 0.00013237991605782686,
      "loss": 2.532,
      "step": 4750
    },
    {
      "epoch": 0.36190907034607556,
      "grad_norm": 1.1448028087615967,
      "learning_rate": 0.00013160267371366394,
      "loss": 2.5895,
      "step": 4800
    },
    {
      "epoch": 0.36567895649551385,
      "grad_norm": 0.37114253640174866,
      "learning_rate": 0.00013082543136950101,
      "loss": 2.617,
      "step": 4850
    },
    {
      "epoch": 0.36944884264495215,
      "grad_norm": 1.15851891040802,
      "learning_rate": 0.00013004818902533812,
      "loss": 2.5274,
      "step": 4900
    },
    {
      "epoch": 0.3732187287943904,
      "grad_norm": 1.0622074604034424,
      "learning_rate": 0.0001292709466811752,
      "loss": 2.4963,
      "step": 4950
    },
    {
      "epoch": 0.3769886149438287,
      "grad_norm": 1.1624069213867188,
      "learning_rate": 0.00012849370433701228,
      "loss": 2.6415,
      "step": 5000
    },
    {
      "epoch": 0.380758501093267,
      "grad_norm": 1.197042465209961,
      "learning_rate": 0.00012771646199284938,
      "loss": 2.5673,
      "step": 5050
    },
    {
      "epoch": 0.38452838724270527,
      "grad_norm": 1.1518622636795044,
      "learning_rate": 0.00012693921964868646,
      "loss": 2.58,
      "step": 5100
    },
    {
      "epoch": 0.38829827339214357,
      "grad_norm": 1.0209661722183228,
      "learning_rate": 0.00012616197730452354,
      "loss": 2.5857,
      "step": 5150
    },
    {
      "epoch": 0.39206815954158186,
      "grad_norm": 1.4015499353408813,
      "learning_rate": 0.00012538473496036064,
      "loss": 2.6275,
      "step": 5200
    },
    {
      "epoch": 0.39583804569102016,
      "grad_norm": 1.021378755569458,
      "learning_rate": 0.00012460749261619772,
      "loss": 2.4994,
      "step": 5250
    },
    {
      "epoch": 0.3996079318404584,
      "grad_norm": 1.371037483215332,
      "learning_rate": 0.00012383025027203483,
      "loss": 2.5751,
      "step": 5300
    },
    {
      "epoch": 0.4033778179898967,
      "grad_norm": 1.230175495147705,
      "learning_rate": 0.0001230530079278719,
      "loss": 2.633,
      "step": 5350
    },
    {
      "epoch": 0.407147704139335,
      "grad_norm": 1.182486891746521,
      "learning_rate": 0.000122275765583709,
      "loss": 2.5083,
      "step": 5400
    },
    {
      "epoch": 0.4109175902887733,
      "grad_norm": 1.310740351676941,
      "learning_rate": 0.00012149852323954609,
      "loss": 2.5443,
      "step": 5450
    },
    {
      "epoch": 0.4146874764382116,
      "grad_norm": 0.8744966387748718,
      "learning_rate": 0.00012072128089538319,
      "loss": 2.5268,
      "step": 5500
    },
    {
      "epoch": 0.4184573625876499,
      "grad_norm": 1.1685783863067627,
      "learning_rate": 0.00011994403855122027,
      "loss": 2.5722,
      "step": 5550
    },
    {
      "epoch": 0.4222272487370881,
      "grad_norm": 0.8275856375694275,
      "learning_rate": 0.00011916679620705737,
      "loss": 2.5741,
      "step": 5600
    },
    {
      "epoch": 0.4259971348865264,
      "grad_norm": 1.4490044116973877,
      "learning_rate": 0.00011838955386289445,
      "loss": 2.4979,
      "step": 5650
    },
    {
      "epoch": 0.4297670210359647,
      "grad_norm": 1.1927125453948975,
      "learning_rate": 0.00011761231151873156,
      "loss": 2.5786,
      "step": 5700
    },
    {
      "epoch": 0.433536907185403,
      "grad_norm": 0.6727531552314758,
      "learning_rate": 0.00011683506917456864,
      "loss": 2.4999,
      "step": 5750
    },
    {
      "epoch": 0.4373067933348413,
      "grad_norm": 1.168397307395935,
      "learning_rate": 0.00011605782683040574,
      "loss": 2.6603,
      "step": 5800
    },
    {
      "epoch": 0.4410766794842796,
      "grad_norm": 1.0242291688919067,
      "learning_rate": 0.00011528058448624282,
      "loss": 2.5632,
      "step": 5850
    },
    {
      "epoch": 0.4448465656337179,
      "grad_norm": 1.0261942148208618,
      "learning_rate": 0.00011450334214207991,
      "loss": 2.5278,
      "step": 5900
    },
    {
      "epoch": 0.4486164517831561,
      "grad_norm": 1.3181779384613037,
      "learning_rate": 0.000113726099797917,
      "loss": 2.4493,
      "step": 5950
    },
    {
      "epoch": 0.4523863379325944,
      "grad_norm": 1.1279304027557373,
      "learning_rate": 0.00011294885745375408,
      "loss": 2.5401,
      "step": 6000
    },
    {
      "epoch": 0.4561562240820327,
      "grad_norm": 1.1858021020889282,
      "learning_rate": 0.00011217161510959117,
      "loss": 2.4279,
      "step": 6050
    },
    {
      "epoch": 0.459926110231471,
      "grad_norm": 1.2025560140609741,
      "learning_rate": 0.00011139437276542825,
      "loss": 2.5571,
      "step": 6100
    },
    {
      "epoch": 0.4636959963809093,
      "grad_norm": 1.0472568273544312,
      "learning_rate": 0.00011061713042126536,
      "loss": 2.5307,
      "step": 6150
    },
    {
      "epoch": 0.4674658825303476,
      "grad_norm": 1.3839603662490845,
      "learning_rate": 0.00010983988807710243,
      "loss": 2.5122,
      "step": 6200
    },
    {
      "epoch": 0.4712357686797859,
      "grad_norm": 1.2016308307647705,
      "learning_rate": 0.00010906264573293954,
      "loss": 2.5456,
      "step": 6250
    },
    {
      "epoch": 0.47500565482922413,
      "grad_norm": 1.1496104001998901,
      "learning_rate": 0.00010828540338877662,
      "loss": 2.548,
      "step": 6300
    },
    {
      "epoch": 0.47877554097866243,
      "grad_norm": 0.9549431800842285,
      "learning_rate": 0.00010750816104461372,
      "loss": 2.5182,
      "step": 6350
    },
    {
      "epoch": 0.4825454271281007,
      "grad_norm": 1.1251375675201416,
      "learning_rate": 0.0001067309187004508,
      "loss": 2.3905,
      "step": 6400
    },
    {
      "epoch": 0.486315313277539,
      "grad_norm": 1.138628363609314,
      "learning_rate": 0.00010596922120317114,
      "loss": 2.5772,
      "step": 6450
    },
    {
      "epoch": 0.4900851994269773,
      "grad_norm": 1.1653454303741455,
      "learning_rate": 0.00010519197885900824,
      "loss": 2.4599,
      "step": 6500
    },
    {
      "epoch": 0.4938550855764156,
      "grad_norm": 1.2576097249984741,
      "learning_rate": 0.00010441473651484532,
      "loss": 2.5859,
      "step": 6550
    },
    {
      "epoch": 0.4976249717258539,
      "grad_norm": 0.95854252576828,
      "learning_rate": 0.00010363749417068243,
      "loss": 2.4624,
      "step": 6600
    },
    {
      "epoch": 0.5013948578752921,
      "grad_norm": 1.1340086460113525,
      "learning_rate": 0.0001028602518265195,
      "loss": 2.4561,
      "step": 6650
    },
    {
      "epoch": 0.5051647440247304,
      "grad_norm": 1.1212513446807861,
      "learning_rate": 0.00010208300948235661,
      "loss": 2.5095,
      "step": 6700
    },
    {
      "epoch": 0.5089346301741687,
      "grad_norm": 1.23517906665802,
      "learning_rate": 0.00010130576713819369,
      "loss": 2.5157,
      "step": 6750
    },
    {
      "epoch": 0.512704516323607,
      "grad_norm": 1.1775014400482178,
      "learning_rate": 0.00010052852479403079,
      "loss": 2.5648,
      "step": 6800
    },
    {
      "epoch": 0.5164744024730453,
      "grad_norm": 1.101165533065796,
      "learning_rate": 9.975128244986788e-05,
      "loss": 2.539,
      "step": 6850
    },
    {
      "epoch": 0.5202442886224836,
      "grad_norm": 1.1235626935958862,
      "learning_rate": 9.897404010570496e-05,
      "loss": 2.5428,
      "step": 6900
    },
    {
      "epoch": 0.5240141747719219,
      "grad_norm": 1.1684298515319824,
      "learning_rate": 9.819679776154205e-05,
      "loss": 2.5198,
      "step": 6950
    },
    {
      "epoch": 0.5277840609213602,
      "grad_norm": 1.4052895307540894,
      "learning_rate": 9.741955541737913e-05,
      "loss": 2.5683,
      "step": 7000
    },
    {
      "epoch": 0.5315539470707985,
      "grad_norm": 0.7240882515907288,
      "learning_rate": 9.664231307321622e-05,
      "loss": 2.5294,
      "step": 7050
    },
    {
      "epoch": 0.5353238332202368,
      "grad_norm": 1.4327476024627686,
      "learning_rate": 9.586507072905332e-05,
      "loss": 2.4559,
      "step": 7100
    },
    {
      "epoch": 0.539093719369675,
      "grad_norm": 0.9776430130004883,
      "learning_rate": 9.508782838489041e-05,
      "loss": 2.574,
      "step": 7150
    },
    {
      "epoch": 0.5428636055191133,
      "grad_norm": 0.8145865797996521,
      "learning_rate": 9.43105860407275e-05,
      "loss": 2.5207,
      "step": 7200
    },
    {
      "epoch": 0.5466334916685516,
      "grad_norm": 1.4643021821975708,
      "learning_rate": 9.353334369656459e-05,
      "loss": 2.4688,
      "step": 7250
    },
    {
      "epoch": 0.5504033778179899,
      "grad_norm": 1.2117152214050293,
      "learning_rate": 9.275610135240168e-05,
      "loss": 2.5602,
      "step": 7300
    },
    {
      "epoch": 0.5541732639674282,
      "grad_norm": 1.415289282798767,
      "learning_rate": 9.197885900823877e-05,
      "loss": 2.3988,
      "step": 7350
    },
    {
      "epoch": 0.5579431501168665,
      "grad_norm": 1.154333472251892,
      "learning_rate": 9.120161666407587e-05,
      "loss": 2.567,
      "step": 7400
    },
    {
      "epoch": 0.5617130362663048,
      "grad_norm": 1.3531889915466309,
      "learning_rate": 9.042437431991296e-05,
      "loss": 2.5317,
      "step": 7450
    },
    {
      "epoch": 0.565482922415743,
      "grad_norm": 1.2713812589645386,
      "learning_rate": 8.964713197575005e-05,
      "loss": 2.6389,
      "step": 7500
    },
    {
      "epoch": 0.5692528085651813,
      "grad_norm": 1.0731208324432373,
      "learning_rate": 8.886988963158714e-05,
      "loss": 2.5608,
      "step": 7550
    },
    {
      "epoch": 0.5730226947146196,
      "grad_norm": 1.3860886096954346,
      "learning_rate": 8.809264728742422e-05,
      "loss": 2.5314,
      "step": 7600
    },
    {
      "epoch": 0.5767925808640579,
      "grad_norm": 1.1006901264190674,
      "learning_rate": 8.731540494326131e-05,
      "loss": 2.5088,
      "step": 7650
    },
    {
      "epoch": 0.5805624670134962,
      "grad_norm": 1.069732427597046,
      "learning_rate": 8.65381625990984e-05,
      "loss": 2.5295,
      "step": 7700
    },
    {
      "epoch": 0.5843323531629345,
      "grad_norm": 0.8163436651229858,
      "learning_rate": 8.576092025493549e-05,
      "loss": 2.4499,
      "step": 7750
    },
    {
      "epoch": 0.5881022393123728,
      "grad_norm": 1.0148953199386597,
      "learning_rate": 8.498367791077258e-05,
      "loss": 2.4621,
      "step": 7800
    },
    {
      "epoch": 0.591872125461811,
      "grad_norm": 1.7218071222305298,
      "learning_rate": 8.420643556660968e-05,
      "loss": 2.5397,
      "step": 7850
    },
    {
      "epoch": 0.5956420116112493,
      "grad_norm": 1.45689058303833,
      "learning_rate": 8.342919322244677e-05,
      "loss": 2.473,
      "step": 7900
    },
    {
      "epoch": 0.5994118977606876,
      "grad_norm": 1.1097421646118164,
      "learning_rate": 8.265195087828386e-05,
      "loss": 2.4585,
      "step": 7950
    },
    {
      "epoch": 0.6031817839101259,
      "grad_norm": 1.1625300645828247,
      "learning_rate": 8.187470853412095e-05,
      "loss": 2.5059,
      "step": 8000
    },
    {
      "epoch": 0.6069516700595642,
      "grad_norm": 1.3960648775100708,
      "learning_rate": 8.111301103684129e-05,
      "loss": 2.5535,
      "step": 8050
    },
    {
      "epoch": 0.6107215562090025,
      "grad_norm": 1.1721075773239136,
      "learning_rate": 8.033576869267838e-05,
      "loss": 2.5114,
      "step": 8100
    },
    {
      "epoch": 0.6144914423584408,
      "grad_norm": 1.0495468378067017,
      "learning_rate": 7.955852634851547e-05,
      "loss": 2.4777,
      "step": 8150
    },
    {
      "epoch": 0.6182613285078791,
      "grad_norm": 1.2005655765533447,
      "learning_rate": 7.878128400435256e-05,
      "loss": 2.5126,
      "step": 8200
    },
    {
      "epoch": 0.6220312146573174,
      "grad_norm": 1.042039394378662,
      "learning_rate": 7.800404166018966e-05,
      "loss": 2.3641,
      "step": 8250
    },
    {
      "epoch": 0.6258011008067557,
      "grad_norm": 1.1380470991134644,
      "learning_rate": 7.722679931602675e-05,
      "loss": 2.4716,
      "step": 8300
    },
    {
      "epoch": 0.629570986956194,
      "grad_norm": 1.0456827878952026,
      "learning_rate": 7.644955697186384e-05,
      "loss": 2.4957,
      "step": 8350
    },
    {
      "epoch": 0.6333408731056323,
      "grad_norm": 1.1709191799163818,
      "learning_rate": 7.567231462770093e-05,
      "loss": 2.383,
      "step": 8400
    },
    {
      "epoch": 0.6371107592550705,
      "grad_norm": 1.05837881565094,
      "learning_rate": 7.489507228353801e-05,
      "loss": 2.4464,
      "step": 8450
    },
    {
      "epoch": 0.6408806454045087,
      "grad_norm": 1.293060541152954,
      "learning_rate": 7.41178299393751e-05,
      "loss": 2.5433,
      "step": 8500
    },
    {
      "epoch": 0.644650531553947,
      "grad_norm": 1.2914859056472778,
      "learning_rate": 7.334058759521219e-05,
      "loss": 2.5193,
      "step": 8550
    },
    {
      "epoch": 0.6484204177033853,
      "grad_norm": 0.6955758929252625,
      "learning_rate": 7.256334525104928e-05,
      "loss": 2.4918,
      "step": 8600
    },
    {
      "epoch": 0.6521903038528236,
      "grad_norm": 1.1332203149795532,
      "learning_rate": 7.178610290688636e-05,
      "loss": 2.4739,
      "step": 8650
    },
    {
      "epoch": 0.6559601900022619,
      "grad_norm": 1.3552063703536987,
      "learning_rate": 7.100886056272345e-05,
      "loss": 2.408,
      "step": 8700
    },
    {
      "epoch": 0.6597300761517002,
      "grad_norm": 1.278549075126648,
      "learning_rate": 7.023161821856054e-05,
      "loss": 2.427,
      "step": 8750
    },
    {
      "epoch": 0.6634999623011385,
      "grad_norm": 1.1147198677062988,
      "learning_rate": 6.945437587439764e-05,
      "loss": 2.546,
      "step": 8800
    },
    {
      "epoch": 0.6672698484505768,
      "grad_norm": 1.17377507686615,
      "learning_rate": 6.867713353023473e-05,
      "loss": 2.4752,
      "step": 8850
    },
    {
      "epoch": 0.6710397346000151,
      "grad_norm": 1.4402492046356201,
      "learning_rate": 6.789989118607182e-05,
      "loss": 2.4883,
      "step": 8900
    },
    {
      "epoch": 0.6748096207494534,
      "grad_norm": 1.244800329208374,
      "learning_rate": 6.712264884190891e-05,
      "loss": 2.452,
      "step": 8950
    },
    {
      "epoch": 0.6785795068988917,
      "grad_norm": 1.1236695051193237,
      "learning_rate": 6.6345406497746e-05,
      "loss": 2.4137,
      "step": 9000
    },
    {
      "epoch": 0.68234939304833,
      "grad_norm": 1.1380460262298584,
      "learning_rate": 6.55681641535831e-05,
      "loss": 2.5472,
      "step": 9050
    },
    {
      "epoch": 0.6861192791977683,
      "grad_norm": 1.1984105110168457,
      "learning_rate": 6.479092180942019e-05,
      "loss": 2.4202,
      "step": 9100
    },
    {
      "epoch": 0.6898891653472066,
      "grad_norm": 1.0361342430114746,
      "learning_rate": 6.401367946525726e-05,
      "loss": 2.4813,
      "step": 9150
    },
    {
      "epoch": 0.6936590514966448,
      "grad_norm": 1.0947997570037842,
      "learning_rate": 6.323643712109436e-05,
      "loss": 2.4884,
      "step": 9200
    },
    {
      "epoch": 0.697428937646083,
      "grad_norm": 1.2177528142929077,
      "learning_rate": 6.245919477693145e-05,
      "loss": 2.4342,
      "step": 9250
    },
    {
      "epoch": 0.7011988237955213,
      "grad_norm": 1.2440896034240723,
      "learning_rate": 6.168195243276854e-05,
      "loss": 2.4691,
      "step": 9300
    },
    {
      "epoch": 0.7049687099449596,
      "grad_norm": 1.1497917175292969,
      "learning_rate": 6.090471008860563e-05,
      "loss": 2.51,
      "step": 9350
    },
    {
      "epoch": 0.7087385960943979,
      "grad_norm": 1.054456114768982,
      "learning_rate": 6.012746774444272e-05,
      "loss": 2.4957,
      "step": 9400
    },
    {
      "epoch": 0.7125084822438362,
      "grad_norm": 1.1272109746932983,
      "learning_rate": 5.9350225400279813e-05,
      "loss": 2.5356,
      "step": 9450
    },
    {
      "epoch": 0.7162783683932745,
      "grad_norm": 0.9659465551376343,
      "learning_rate": 5.8572983056116905e-05,
      "loss": 2.3779,
      "step": 9500
    },
    {
      "epoch": 0.7200482545427128,
      "grad_norm": 1.2713148593902588,
      "learning_rate": 5.7795740711954e-05,
      "loss": 2.4356,
      "step": 9550
    },
    {
      "epoch": 0.7238181406921511,
      "grad_norm": 1.5280075073242188,
      "learning_rate": 5.701849836779108e-05,
      "loss": 2.4852,
      "step": 9600
    },
    {
      "epoch": 0.7275880268415894,
      "grad_norm": 1.3369061946868896,
      "learning_rate": 5.624125602362817e-05,
      "loss": 2.3641,
      "step": 9650
    },
    {
      "epoch": 0.7313579129910277,
      "grad_norm": 1.0855350494384766,
      "learning_rate": 5.5464013679465265e-05,
      "loss": 2.4128,
      "step": 9700
    },
    {
      "epoch": 0.735127799140466,
      "grad_norm": 1.0044677257537842,
      "learning_rate": 5.468677133530234e-05,
      "loss": 2.4228,
      "step": 9750
    },
    {
      "epoch": 0.7388976852899043,
      "grad_norm": 1.210113525390625,
      "learning_rate": 5.3909528991139434e-05,
      "loss": 2.3754,
      "step": 9800
    },
    {
      "epoch": 0.7426675714393425,
      "grad_norm": 1.1125415563583374,
      "learning_rate": 5.3132286646976526e-05,
      "loss": 2.3915,
      "step": 9850
    },
    {
      "epoch": 0.7464374575887808,
      "grad_norm": 1.3552277088165283,
      "learning_rate": 5.235504430281362e-05,
      "loss": 2.4077,
      "step": 9900
    },
    {
      "epoch": 0.7502073437382191,
      "grad_norm": 1.038862705230713,
      "learning_rate": 5.157780195865071e-05,
      "loss": 2.527,
      "step": 9950
    },
    {
      "epoch": 0.7539772298876574,
      "grad_norm": 1.2627066373825073,
      "learning_rate": 5.0800559614487794e-05,
      "loss": 2.4311,
      "step": 10000
    },
    {
      "epoch": 0.7577471160370957,
      "grad_norm": 1.4121466875076294,
      "learning_rate": 5.0023317270324886e-05,
      "loss": 2.4427,
      "step": 10050
    },
    {
      "epoch": 0.761517002186534,
      "grad_norm": 1.2718158960342407,
      "learning_rate": 4.924607492616198e-05,
      "loss": 2.5851,
      "step": 10100
    },
    {
      "epoch": 0.7652868883359722,
      "grad_norm": 1.1576273441314697,
      "learning_rate": 4.846883258199907e-05,
      "loss": 2.4353,
      "step": 10150
    },
    {
      "epoch": 0.7690567744854105,
      "grad_norm": 0.89919114112854,
      "learning_rate": 4.769159023783616e-05,
      "loss": 2.4291,
      "step": 10200
    },
    {
      "epoch": 0.7728266606348488,
      "grad_norm": 1.2037330865859985,
      "learning_rate": 4.691434789367325e-05,
      "loss": 2.441,
      "step": 10250
    },
    {
      "epoch": 0.7765965467842871,
      "grad_norm": 1.4790791273117065,
      "learning_rate": 4.613710554951034e-05,
      "loss": 2.5034,
      "step": 10300
    },
    {
      "epoch": 0.7803664329337254,
      "grad_norm": 0.564732015132904,
      "learning_rate": 4.535986320534743e-05,
      "loss": 2.4255,
      "step": 10350
    },
    {
      "epoch": 0.7841363190831637,
      "grad_norm": 1.1132522821426392,
      "learning_rate": 4.458262086118452e-05,
      "loss": 2.4557,
      "step": 10400
    },
    {
      "epoch": 0.787906205232602,
      "grad_norm": 0.7629557251930237,
      "learning_rate": 4.380537851702161e-05,
      "loss": 2.4608,
      "step": 10450
    },
    {
      "epoch": 0.7916760913820403,
      "grad_norm": 1.0398045778274536,
      "learning_rate": 4.3028136172858703e-05,
      "loss": 2.49,
      "step": 10500
    },
    {
      "epoch": 0.7954459775314785,
      "grad_norm": 1.105462908744812,
      "learning_rate": 4.225089382869579e-05,
      "loss": 2.3739,
      "step": 10550
    },
    {
      "epoch": 0.7992158636809168,
      "grad_norm": 1.184937596321106,
      "learning_rate": 4.147365148453288e-05,
      "loss": 2.4433,
      "step": 10600
    },
    {
      "epoch": 0.8029857498303551,
      "grad_norm": 0.9720554947853088,
      "learning_rate": 4.0696409140369965e-05,
      "loss": 2.4107,
      "step": 10650
    },
    {
      "epoch": 0.8067556359797934,
      "grad_norm": 1.3953956365585327,
      "learning_rate": 3.9919166796207056e-05,
      "loss": 2.433,
      "step": 10700
    },
    {
      "epoch": 0.8105255221292317,
      "grad_norm": 1.4250519275665283,
      "learning_rate": 3.914192445204415e-05,
      "loss": 2.4663,
      "step": 10750
    },
    {
      "epoch": 0.81429540827867,
      "grad_norm": 1.3815895318984985,
      "learning_rate": 3.836468210788124e-05,
      "loss": 2.3827,
      "step": 10800
    },
    {
      "epoch": 0.8180652944281083,
      "grad_norm": 0.7326750755310059,
      "learning_rate": 3.758743976371833e-05,
      "loss": 2.434,
      "step": 10850
    },
    {
      "epoch": 0.8218351805775466,
      "grad_norm": 1.059107780456543,
      "learning_rate": 3.681019741955542e-05,
      "loss": 2.3788,
      "step": 10900
    },
    {
      "epoch": 0.8256050667269849,
      "grad_norm": 1.5107840299606323,
      "learning_rate": 3.603295507539251e-05,
      "loss": 2.4756,
      "step": 10950
    },
    {
      "epoch": 0.8293749528764232,
      "grad_norm": 0.5051243305206299,
      "learning_rate": 3.52557127312296e-05,
      "loss": 2.4848,
      "step": 11000
    },
    {
      "epoch": 0.8331448390258615,
      "grad_norm": 1.1853724718093872,
      "learning_rate": 3.447847038706669e-05,
      "loss": 2.3885,
      "step": 11050
    },
    {
      "epoch": 0.8369147251752997,
      "grad_norm": 1.0292357206344604,
      "learning_rate": 3.3701228042903776e-05,
      "loss": 2.2941,
      "step": 11100
    },
    {
      "epoch": 0.840684611324738,
      "grad_norm": 1.3643662929534912,
      "learning_rate": 3.292398569874087e-05,
      "loss": 2.4036,
      "step": 11150
    },
    {
      "epoch": 0.8444544974741762,
      "grad_norm": 1.0186501741409302,
      "learning_rate": 3.214674335457796e-05,
      "loss": 2.4438,
      "step": 11200
    },
    {
      "epoch": 0.8482243836236145,
      "grad_norm": 1.131868600845337,
      "learning_rate": 3.136950101041505e-05,
      "loss": 2.3819,
      "step": 11250
    },
    {
      "epoch": 0.8519942697730528,
      "grad_norm": 1.3239765167236328,
      "learning_rate": 3.0592258666252135e-05,
      "loss": 2.5004,
      "step": 11300
    },
    {
      "epoch": 0.8557641559224911,
      "grad_norm": 0.5920374989509583,
      "learning_rate": 2.9815016322089227e-05,
      "loss": 2.4316,
      "step": 11350
    },
    {
      "epoch": 0.8595340420719294,
      "grad_norm": 0.7451714277267456,
      "learning_rate": 2.903777397792632e-05,
      "loss": 2.4282,
      "step": 11400
    },
    {
      "epoch": 0.8633039282213677,
      "grad_norm": 1.110545039176941,
      "learning_rate": 2.826053163376341e-05,
      "loss": 2.4704,
      "step": 11450
    },
    {
      "epoch": 0.867073814370806,
      "grad_norm": 1.019291639328003,
      "learning_rate": 2.74832892896005e-05,
      "loss": 2.4936,
      "step": 11500
    },
    {
      "epoch": 0.8708437005202443,
      "grad_norm": 1.3071296215057373,
      "learning_rate": 2.670604694543759e-05,
      "loss": 2.3548,
      "step": 11550
    },
    {
      "epoch": 0.8746135866696826,
      "grad_norm": 0.7020739912986755,
      "learning_rate": 2.592880460127468e-05,
      "loss": 2.4757,
      "step": 11600
    },
    {
      "epoch": 0.8783834728191209,
      "grad_norm": 0.9241279363632202,
      "learning_rate": 2.5151562257111767e-05,
      "loss": 2.4307,
      "step": 11650
    },
    {
      "epoch": 0.8821533589685592,
      "grad_norm": 1.334829330444336,
      "learning_rate": 2.4374319912948858e-05,
      "loss": 2.4015,
      "step": 11700
    },
    {
      "epoch": 0.8859232451179975,
      "grad_norm": 1.279158353805542,
      "learning_rate": 2.359707756878595e-05,
      "loss": 2.3864,
      "step": 11750
    },
    {
      "epoch": 0.8896931312674358,
      "grad_norm": 1.076140284538269,
      "learning_rate": 2.2819835224623038e-05,
      "loss": 2.5085,
      "step": 11800
    },
    {
      "epoch": 0.8934630174168741,
      "grad_norm": 1.2217904329299927,
      "learning_rate": 2.2042592880460126e-05,
      "loss": 2.5123,
      "step": 11850
    },
    {
      "epoch": 0.8972329035663122,
      "grad_norm": 1.3986823558807373,
      "learning_rate": 2.1265350536297218e-05,
      "loss": 2.4778,
      "step": 11900
    },
    {
      "epoch": 0.9010027897157505,
      "grad_norm": 1.2675954103469849,
      "learning_rate": 2.048810819213431e-05,
      "loss": 2.4062,
      "step": 11950
    },
    {
      "epoch": 0.9047726758651888,
      "grad_norm": 1.183274507522583,
      "learning_rate": 1.9710865847971398e-05,
      "loss": 2.411,
      "step": 12000
    },
    {
      "epoch": 0.9085425620146271,
      "grad_norm": 1.1216435432434082,
      "learning_rate": 1.893362350380849e-05,
      "loss": 2.3991,
      "step": 12050
    },
    {
      "epoch": 0.9123124481640654,
      "grad_norm": 1.1605224609375,
      "learning_rate": 1.8156381159645577e-05,
      "loss": 2.4106,
      "step": 12100
    },
    {
      "epoch": 0.9160823343135037,
      "grad_norm": 1.2380177974700928,
      "learning_rate": 1.737913881548267e-05,
      "loss": 2.4837,
      "step": 12150
    },
    {
      "epoch": 0.919852220462942,
      "grad_norm": 1.0431979894638062,
      "learning_rate": 1.6601896471319757e-05,
      "loss": 2.4899,
      "step": 12200
    },
    {
      "epoch": 0.9236221066123803,
      "grad_norm": 1.0403482913970947,
      "learning_rate": 1.582465412715685e-05,
      "loss": 2.4593,
      "step": 12250
    },
    {
      "epoch": 0.9273919927618186,
      "grad_norm": 1.2774393558502197,
      "learning_rate": 1.5047411782993939e-05,
      "loss": 2.5494,
      "step": 12300
    },
    {
      "epoch": 0.9311618789112569,
      "grad_norm": 1.2691049575805664,
      "learning_rate": 1.4270169438831027e-05,
      "loss": 2.4987,
      "step": 12350
    },
    {
      "epoch": 0.9349317650606952,
      "grad_norm": 0.9747609496116638,
      "learning_rate": 1.3508471941551375e-05,
      "loss": 2.4632,
      "step": 12400
    },
    {
      "epoch": 0.9387016512101335,
      "grad_norm": 1.2664546966552734,
      "learning_rate": 1.2731229597388467e-05,
      "loss": 2.3734,
      "step": 12450
    },
    {
      "epoch": 0.9424715373595718,
      "grad_norm": 1.0301506519317627,
      "learning_rate": 1.1953987253225555e-05,
      "loss": 2.4497,
      "step": 12500
    },
    {
      "epoch": 0.9462414235090101,
      "grad_norm": 1.138250708580017,
      "learning_rate": 1.1176744909062647e-05,
      "loss": 2.4747,
      "step": 12550
    },
    {
      "epoch": 0.9500113096584483,
      "grad_norm": 1.0994607210159302,
      "learning_rate": 1.0399502564899737e-05,
      "loss": 2.3522,
      "step": 12600
    },
    {
      "epoch": 0.9537811958078866,
      "grad_norm": 1.1914631128311157,
      "learning_rate": 9.622260220736825e-06,
      "loss": 2.4547,
      "step": 12650
    },
    {
      "epoch": 0.9575510819573249,
      "grad_norm": 1.241470456123352,
      "learning_rate": 8.845017876573917e-06,
      "loss": 2.4618,
      "step": 12700
    },
    {
      "epoch": 0.9613209681067632,
      "grad_norm": 0.9680754542350769,
      "learning_rate": 8.067775532411006e-06,
      "loss": 2.5043,
      "step": 12750
    },
    {
      "epoch": 0.9650908542562014,
      "grad_norm": 1.3262577056884766,
      "learning_rate": 7.2905331882480955e-06,
      "loss": 2.3342,
      "step": 12800
    },
    {
      "epoch": 0.9688607404056397,
      "grad_norm": 1.0585600137710571,
      "learning_rate": 6.513290844085186e-06,
      "loss": 2.4323,
      "step": 12850
    },
    {
      "epoch": 0.972630626555078,
      "grad_norm": 1.1000913381576538,
      "learning_rate": 5.736048499922276e-06,
      "loss": 2.3997,
      "step": 12900
    },
    {
      "epoch": 0.9764005127045163,
      "grad_norm": 1.2249661684036255,
      "learning_rate": 4.958806155759365e-06,
      "loss": 2.329,
      "step": 12950
    },
    {
      "epoch": 0.9801703988539546,
      "grad_norm": 2.021547555923462,
      "learning_rate": 4.181563811596456e-06,
      "loss": 2.4766,
      "step": 13000
    },
    {
      "epoch": 0.9839402850033929,
      "grad_norm": 1.4664599895477295,
      "learning_rate": 3.404321467433546e-06,
      "loss": 2.4165,
      "step": 13050
    },
    {
      "epoch": 0.9877101711528312,
      "grad_norm": 0.8574293255805969,
      "learning_rate": 2.627079123270636e-06,
      "loss": 2.4594,
      "step": 13100
    },
    {
      "epoch": 0.9914800573022695,
      "grad_norm": 0.9103701114654541,
      "learning_rate": 1.8498367791077258e-06,
      "loss": 2.4705,
      "step": 13150
    },
    {
      "epoch": 0.9952499434517078,
      "grad_norm": 1.1798721551895142,
      "learning_rate": 1.0725944349448157e-06,
      "loss": 2.3305,
      "step": 13200
    },
    {
      "epoch": 0.999019829601146,
      "grad_norm": 1.397168517112732,
      "learning_rate": 2.953520907819058e-07,
      "loss": 2.453,
      "step": 13250
    }
  ],
  "logging_steps": 50,
  "max_steps": 13263,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 7.675538781892608e+16,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
